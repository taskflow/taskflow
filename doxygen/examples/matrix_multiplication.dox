namespace tf {

/** @page matrix_multiplication Matrix Multiplication

We study the classic problem, <em>2D matrix multiplication</em>. 
We will start with a short introduction about the problem and then discuss how to solve it using
CPU and GPU parallel computing.

@tableofcontents

@section MatrixMultiplicationProblem Problem Formulation

We are multiplying two matrices, A (MxK) and B (KxN).
The numbers of columns of A must match the number of rows of B. 
The output matrix C has the shape of (MxN) where M is the rows of A and N the columns of B.
The following example multiplies a 3x3 matrix with a 3x2 matrix to derive
a 3x2 matrix.

@image html images/matrix_multiplication_1.png width=50%

As a general view, 
for each element of C we iterate a complete row of A and  a complete column of B, 
multiplying each element and summing them.

@image html images/matrix_multiplication_2.png width=50%

We can implement matrix multiplication using three nested loops.

@code{.cpp}
for(int m=0; m<M; m++) {
  for(int n=0; n<N; n++) {
    C[m][n] = 0;
    for(int k=0; k<K; k++) {
      C[m][n] += A[m][k] * B[k][n];
    }
  }
}
@endcode

@section MatrixMultiplicationParallelPattern Parallel Patterns

At a fine-grained level, computing each element of C is independent of each other.
Similarly, computing each row of C or each column of C is also independent of one another.
With task parallelism, we prefer <em>coarse-grained</em> model 
to have each task perform rather large computation to amortize
the overhead of creating and scheduling tasks.
In this case, we avoid intensive tasks each working on only a single element.
by creating a task per row of C 
to multiply a row of A by every column of B.

@code{.cpp}
// C = A * B
// A is a MxK matrix, B is a KxN matrix, and C is a MxN matrix
void matrix_multiplication(int** A, int** B, int** C, int M, int K, int N) {

  tf::Taskflow taskflow;
  tf::Executor executor;
  
  for(int m=0; m<M; ++m) {
    taskflow.emplace([m, &] () {
      for(int n=0; n<N; n++) {
        for(int k=0; k<K; k++) {
          C[m][n] += A[m][k] * B[k][n];
        }
      }
    });
  }

  executor.run(taskflow).wait();
}
@endcode

Instead of creating tasks one-by-one over a loop,
you can leverage Taskflow::for_each_index to create a <em>parallel-for</em> task.
A parallel-for task spawns a subflow to perform parallel iterations over the given range.

@code{.cpp}
// perform parallel iterations on the range [0, M) with the step size of 1
tf::Task task = taskflow.for_each_index(0, M, 1, [&] (int m) {
  for(int n=0; n<N; n++) {
    for(int k=0; k<K; k++) {
      C[m][n] += A[m][k] * B[k][n];
    }   
  }   
}); 
@endcode

Please visit @ref A1ForEach for more details.

@section GPUAcceleratedMatrixMultiplication GPU-based Acceleration

GPU is able to do a lot of parallel computations more than CPUs.
It is especially useful for data-intensive computing such as matrix multiplication.
With GPU, we express the parallel patterns at a fine-grained level.
The kernel, written in CUDA, is described as follows:

@code{.cpp}
// CUDA kernel to perform matrix multiplication
__global__ void matmul(int *A, int *B, int *C, int M, int K, int N) {
  int row = blockIdx.y * blockDim.y + threadIdx.y;
  int col = blockIdx.x * blockDim.x + threadIdx.x;
  int sum = 0;
  if(col < N && row < M) {
    for(int i = 0; i < K; i++) {
      sum += a[row * K + i] * b[i * N + col];
    }
    c[row * N + col] = sum;
  }
}
@endcode

Each CUDA thread corresponds to an element of C and compute its result.
Instead of storing each matrix in a 2D array, 
we use 1D layout to ease the data transfer between CPU and GPU.
In a row-major layout, an element <tt>(x, y)</tt> in the 2D matrix 
can be addressed at <tt>x * width + y</tt> in the transformed 1D layout.

@image html images/matrix_multiplication_4.png width=70%

The next step is to allocate memory for A, B, and C at a GPU.
We create three tasks each calling @c cudaMalloc to allocate space for one matrix.
Then, we create a %cudaFlow to offload matrix multiplication to a GPU.
The entire code is described as follows:

@code{.cpp}
void matrix_multiplication(int* A, int* B, int* C, int M, int K, int N) {
  
  tf::Taskflow taskflow;
  tf::Executor executor;

  // allocate the host and gpu storage for A
  tf::Task allocate_a = taskflow.emplace([&](){
    cudaMalloc(&da, M*K*sizeof(int));
  }).name("allocate_a");
  
  // allocate the host and gpu storage for B
  tf::Task allocate_b = taskflow.emplace([&](){
    cudaMalloc(&db, K*N*sizeof(int));
  }).name("allocate_b");
  
  // allocate the host and gpu storage for C
  tf::Task allocate_c = taskflow.emplace([&](){
    cudaMalloc(&dc, M*N*sizeof(int));
  }).name("allocate_c");
  
  // create a cudaFlow to run the matrix multiplication
  tf::Task cudaFlow = taskflow.emplace([&](tf::cudaFlow& cf){
  
    // copy data to da, db, and dc
    tf::cudaTask copy_da = cf.copy(da, A, M*K).name("H2D_A");
    tf::cudaTask copy_db = cf.copy(db, B, K*N).name("H2D_B");
    tf::cudaTask copy_hc = cf.copy(C, dc, M*N).name("D2H_C");
  
    dim3 grid  ((K+16-1)/16, (M+16-1)/16);
    dim3 block (16, 16);
  
    tf::cudaTask kmatmul = cf.kernel(grid, block, 0, matmul, da, db, dc, M, K, N)
                             .name("matmul");
  
    kmatmul.succeed(copy_da, copy_db)
           .precede(copy_hc);
  
  }).name("cudaFlow");
  
  // free the gpu storage
  auto free = taskflow.emplace([&](){
    cudaFree(da);
    cudaFree(db);
    cudaFree(dc);
  }).name("free");
  
  // create dependency
  cudaFlow.succeed(allocate_a, allocate_b, allocate_c)
          .precede(free);
  
  // dump the graph without unfolding the cudaFlow
  taskflow.dump(std::cout);

  // run the taskflow
  executor.run(taskflow).wait();

  // dump the entire execution graph including unfolded cudaFlow
  taskflow.dump(std::cout);
}
@endcode

Within the %cudaFlow, we create two host-to-device (H2D) tasks that copy data from @c A and @c B
to @c da and @c db,
one device-to-host (D2H) task that copies the result from @c dc to @c C,
and one kernel task that launches @c matmul on the GPU
(by default, GPU 0).
H2D tasks precede the kernel and the kernel precedes the D2H task.
These GPU operations form a GPU task graph managed by a %cudaFlow.
The first dump of the taskflow gives the following graph:

<!-- @image html images/matrix_multiplication_5.svg width=40% -->
@dotfile images/matrix_multiplication_5.dot

A %cudaFlow encapsulates a GPU task dependency graph similar to a subflow (see @ref DynamicTasking). 
In order to visualize it, we need to execute 
the graph first and then dump the taskflow.

<!-- @image html images/matrix_multiplication_6.svg width=50% -->
@dotfile images/matrix_multiplication_6.dot

@section MatrixMultiplicationBenchmarking Benchmarking

We run three versions of matrix multiplication,
sequential CPU, parallel CPUs, and one GPU,
on a machine of 6 Intel i7-8700 CPUs at 3.20GHz and a Nvidia RTX 2080 GPU using
various matrix sizes of A, B, and C.

<div align="center">
| A         | B         | C         | CPU Sequential | CPU Parallel | GPU Parallel |
| :-:       | :-:       | :-:       | :-:            | :-:          | :-:          |
| 10x10     | 10x10     | 10x10     | 0.142 ms       | 0.414 ms     | 82 ms        |
| 100x100   | 100x100   | 100x100   | 1.641 ms       | 0.733 ms     | 83 ms        |
| 1000x1000 | 1000x1000 | 1000x1000 | 1532  ms       | 504 ms       | 85 ms        |
| 2000x2000 | 2000x2000 | 2000x2000 | 25688 ms       | 4387 ms      | 133 ms       |
| 3000x3000 | 3000x3000 | 3000x3000 | 104838 ms      | 16170 ms     | 214 ms       |
| 4000x4000 | 4000x4000 | 4000x4000 | 250133 ms      | 39646 ms     | 427 ms       |
</div>

With the matrix size going up to 1000,
the speed-up of GPU over CPUs becomes prominent.

*/

}











