<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Cookbook &raquo; GPU Tasking | Taskflow QuickStart</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,600i%7CSource+Code+Pro:400,400i,600" />
  <link rel="stylesheet" href="m-dark+documentation.compiled.css" />
  <link rel="icon" href="favicon.ico" type="image/x-icon" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#22272e" />
</head>
<body>
<header><nav id="navigation">
  <div class="m-container">
    <div class="m-row">
      <span id="m-navbar-brand" class="m-col-t-8 m-col-m-none m-left-m">
        <a href="https://taskflow.github.io"><img src="taskflow_logo.png" alt="" />Taskflow</a> <span class="m-breadcrumb">|</span> <a href="index.html" class="m-thin">QuickStart</a>
      </span>
      <div class="m-col-t-4 m-hide-m m-text-right m-nopadr">
        <a href="#search" class="m-doc-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
          <path id="m-doc-search-icon-path" d="m6 0c-3.31 0-6 2.69-6 6 0 3.31 2.69 6 6 6 1.49 0 2.85-0.541 3.89-1.44-0.0164 0.338 0.147 0.759 0.5 1.15l3.22 3.79c0.552 0.614 1.45 0.665 2 0.115 0.55-0.55 0.499-1.45-0.115-2l-3.79-3.22c-0.392-0.353-0.812-0.515-1.15-0.5 0.895-1.05 1.44-2.41 1.44-3.89 0-3.31-2.69-6-6-6zm0 1.56a4.44 4.44 0 0 1 4.44 4.44 4.44 4.44 0 0 1-4.44 4.44 4.44 4.44 0 0 1-4.44-4.44 4.44 4.44 0 0 1 4.44-4.44z"/>
        </svg></a>
        <a id="m-navbar-show" href="#navigation" title="Show navigation"></a>
        <a id="m-navbar-hide" href="#" title="Hide navigation"></a>
      </div>
      <div id="m-navbar-collapse" class="m-col-t-12 m-show-m m-col-m-none m-right-m">
        <div class="m-row">
          <ol class="m-col-t-6 m-col-m-none">
            <li><a href="pages.html">Handbook</a></li>
            <li><a href="namespaces.html">Namespaces</a></li>
          </ol>
          <ol class="m-col-t-6 m-col-m-none" start="3">
            <li><a href="annotated.html">Classes</a></li>
            <li><a href="files.html">Files</a></li>
            <li class="m-show-m"><a href="#search" class="m-doc-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
              <use href="#m-doc-search-icon-path" />
            </svg></a></li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</nav></header>
<main><article>
  <div class="m-container m-container-inflatable">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <h1>
          <span class="m-breadcrumb"><a href="Cookbook.html">Cookbook</a> &raquo;</span>
          GPU Tasking
        </h1>
        <nav class="m-block m-default">
          <h3>Contents</h3>
          <ul>
            <li><a href="#GPUTaskingIncludeTheHeader">Include the Header</a></li>
            <li><a href="#WhatIsACudaGraph">What is a CUDA Graph?</a></li>
            <li><a href="#CreateACUDAGraph">Create a CUDA Graph</a></li>
            <li><a href="#CompileACUDAGraphProgram">Compile a CUDA Graph Program</a></li>
            <li><a href="#RunACUDAGraphOnASpecificGPU">Run a CUDA Graph on Specific GPU</a></li>
            <li><a href="#GPUMemoryOperations">Create Memory Operation Tasks</a></li>
            <li><a href="#RunACUDAGraph">Run a CUDA Graph</a></li>
            <li><a href="#UpdateAnExecutableCUDAGraph">Update an Executable CUDA Graph</a></li>
            <li><a href="#IntegrateACUDAGraphIntoTaskflow">Integrate a CUDA Graph into Taskflow</a></li>
          </ul>
        </nav>
<p>Modern scientific computing typically leverages GPU-powered parallel processing cores to speed up large-scale applications. This chapter discusses how to implement CPU-GPU heterogeneous tasking algorithms with Nvidia <a href="https://developer.nvidia.com/blog/cuda-graphs/">CUDA Graph</a>.</p><section id="GPUTaskingIncludeTheHeader"><h2><a href="#GPUTaskingIncludeTheHeader">Include the Header</a></h2><p>You need to include the header file, <code>taskflow/cuda/cudaflow.hpp</code>, for creating a GPU task graph using <a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a>.</p><pre class="m-code"><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;taskflow/cuda/cudaflow.hpp&gt;</span></pre></section><section id="WhatIsACudaGraph"><h2><a href="#WhatIsACudaGraph">What is a CUDA Graph?</a></h2><p>CUDA Graph is a new execution model that enables a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a task graph of operations, rather than a sequence of individually-launched operations. This organization allows launching multiple GPU operations through a single CPU operation and hence reduces the launching overheads, especially for kernels of short running time. The benefit of CUDA Graph can be demonstrated in the figure below:</p><img class="m-image" src="cuda_graph_benefit.png" alt="Image" /><p>In this example, a sequence of short kernels is launched one-by-one by the CPU. The CPU launching overhead creates a significant gap in between the kernels. If we replace this sequence of kernels with a CUDA graph, initially we will need to spend a little extra time on building the graph and launching the whole graph in one go on the first occasion, but subsequent executions will be very fast, as there will be very little gap between the kernels. The difference is more pronounced when the same sequence of operations is repeated many times, for example, many training epochs in machine learning workloads. In that case, the initial costs of building and launching the graph will be amortized over the entire training iterations.</p><aside class="m-note m-warning"><h4>Attention</h4><p>A comprehensive introduction about CUDA Graph can be referred to the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs">CUDA Graph Programming Guide</a>.</p></aside></section><section id="CreateACUDAGraph"><h2><a href="#CreateACUDAGraph">Create a CUDA Graph</a></h2><p>Taskflow leverages <a href="https://developer.nvidia.com/blog/cuda-graphs/">CUDA Graph</a> to enable concurrent CPU-GPU tasking using a task graph model called <a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a>. A <a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a> is essentially a C++ wrapper over a native CUDA graph, designed to simplify GPU task graph programming by eliminating much of the boilerplate code required in raw CUDA Graph programming. The following example creates a CUDA graph to perform the saxpy (A·X Plus Y) workload:</p><pre class="m-code"><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;taskflow/cuda/cudaflow.hpp&gt;</span>

<span class="c1">// saxpy (single-precision A·X Plus Y) kernel</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">saxpy</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// main function begins</span>
<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">unsigned</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span><span class="w">                            </span><span class="c1">// size of the vector</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hx</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">);</span><span class="w">                      </span><span class="c1">// x vector at host</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hy</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="mf">2.0f</span><span class="p">);</span><span class="w">                      </span><span class="c1">// y vector at host</span>

<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">dx</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span><span class="w">                                  </span><span class="c1">// x vector at device</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">dy</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span><span class="w">                                  </span><span class="c1">// y vector at device</span>
<span class="w"> </span>
<span class="w">  </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dx</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dy</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaGraph</span><span class="w"> </span><span class="n">cg</span><span class="p">;</span>
<span class="w">  </span>
<span class="w">  </span><span class="c1">// create data transfer tasks</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span><span class="w"> </span><span class="n">h2d_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span><span class="w"> </span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">N</span><span class="p">);</span><span class="w"> </span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span><span class="w"> </span><span class="n">h2d_y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span><span class="w"> </span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span><span class="w"> </span><span class="n">d2h_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">dx</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span><span class="w"> </span><span class="n">d2h_y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">dy</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// launch saxpy&lt;&lt;&lt;(N+255)/256, 256, 0&gt;&gt;&gt;(N, 2.0f, dx, dy)</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span>
<span class="w">    </span><span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">saxpy</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="mf">2.0f</span><span class="p">,</span><span class="w"> </span><span class="n">dx</span><span class="p">,</span><span class="w"> </span><span class="n">dy</span>
<span class="w">  </span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>

<span class="w">  </span><span class="n">kernel</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span><span class="w"> </span><span class="n">h2d_y</span><span class="p">)</span>
<span class="w">        </span><span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span><span class="w"> </span><span class="n">d2h_y</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// instantiate a CUDA graph executable and run it through a stream</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaGraphExec</span><span class="w"> </span><span class="nf">ecec</span><span class="p">(</span><span class="n">cg</span><span class="p">);</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaStream</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="w">  </span><span class="n">stream</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">exec</span><span class="p">).</span><span class="n">synchronize</span><span class="p">();</span>
<span class="w">  </span>
<span class="w">  </span><span class="c1">// dump the graph</span>
<span class="w">  </span><span class="n">cg</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="p">);</span>
<span class="p">}</span></pre><p>The graph consists of two CPU-to-GPU data copies (<code>h2d_x</code> and <code>h2d_y</code>), one kernel (<code>saxpy</code>), and two GPU-to-CPU data copies (<code>d2h_x</code> and <code>d2h_y</code>), in this order of their task dependencies.</p><div class="m-graph"><svg style="width: 24.200rem; height: 9.800rem;" viewBox="0.00 0.00 242.00 98.00">
<g transform="scale(1 1) rotate(0) translate(4 94)">
<title>Taskflow</title>
<g class="m-node m-flat">
<title>p0x7f2870401a50</title>
<ellipse cx="27" cy="-72" rx="27" ry="18"/>
<text text-anchor="middle" x="27" y="-68.12" font-family="Helvetica,sans-Serif" font-size="10.00">h2d_x</text>
</g>
<g class="m-node">
<title>p0x7f2870402bc0</title>
<polygon points="144,-63 94,-63 90,-59 90,-27 140,-27 144,-31 144,-63"/>
<polyline points="140,-59 90,-59"/>
<polyline points="140,-59 140,-27"/>
<polyline points="140,-59 144,-63"/>
<text text-anchor="middle" x="117" y="-41.12" font-family="Helvetica,sans-Serif" font-size="10.00" fill="white">saxpy</text>
</g>
<g class="m-edge">
<title>p0x7f2870401a50&#45;&gt;p0x7f2870402bc0</title>
<path d="M52.05,-64.62C60.3,-62.09 69.73,-59.2 78.78,-56.42"/>
<polygon points="79.53,-59.85 88.06,-53.57 77.47,-53.16 79.53,-59.85"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402310</title>
<ellipse cx="207" cy="-72" rx="27" ry="18"/>
<text text-anchor="middle" x="207" y="-68.12" font-family="Helvetica,sans-Serif" font-size="10.00">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402310</title>
<path d="M144.4,-53.1C152.8,-55.68 162.23,-58.57 171.13,-61.3"/>
<polygon points="169.98,-64.61 180.57,-64.2 172.03,-57.92 169.98,-64.61"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402780</title>
<ellipse cx="207" cy="-18" rx="27" ry="18"/>
<text text-anchor="middle" x="207" y="-14.12" font-family="Helvetica,sans-Serif" font-size="10.00">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402780</title>
<path d="M144.4,-36.9C152.8,-34.32 162.23,-31.43 171.13,-28.7"/>
<polygon points="172.03,-32.08 180.57,-25.8 169.98,-25.39 172.03,-32.08"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870401eb0</title>
<ellipse cx="27" cy="-18" rx="27" ry="18"/>
<text text-anchor="middle" x="27" y="-14.12" font-family="Helvetica,sans-Serif" font-size="10.00">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870401eb0&#45;&gt;p0x7f2870402bc0</title>
<path d="M52.05,-25.38C60.3,-27.91 69.73,-30.8 78.78,-33.58"/>
<polygon points="77.47,-36.84 88.06,-36.43 79.53,-30.15 77.47,-36.84"/>
</g>
</g>
</svg>
</div><p>We do not expend yet another effort on simplifying kernel programming but focus on tasking CUDA operations and their dependencies. That is, <a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a> is simply a lightweight C++ wrapper over the native CUDA Graph. This organization lets users fully take advantage of CUDA features that are commensurate with their domain knowledge, while leaving difficult task parallelism details to Taskflow.</p></section><section id="CompileACUDAGraphProgram"><h2><a href="#CompileACUDAGraphProgram">Compile a CUDA Graph Program</a></h2><p>Use <a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">nvcc</a> to compile a CUDA Graph program:</p><pre class="m-code">~$<span class="w"> </span>nvcc<span class="w"> </span>-std<span class="o">=</span>c++20<span class="w"> </span>my_cudaflow.cu<span class="w"> </span>-I<span class="w"> </span>path/to/include/taskflow<span class="w"> </span>-O2<span class="w"> </span>-o<span class="w"> </span>my_cudaflow
~$<span class="w"> </span>./my_cudaflow</pre><p>Please visit the page <a href="CompileTaskflowWithCUDA.html" class="m-doc">Compile Taskflow with CUDA</a> for more details.</p></section><section id="RunACUDAGraphOnASpecificGPU"><h2><a href="#RunACUDAGraphOnASpecificGPU">Run a CUDA Graph on Specific GPU</a></h2><p>By default, a <a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a> runs on the current GPU context associated with the caller, which is typically GPU <code>0</code>. Each CUDA GPU has an integer identifier in the range of <code>[0, N)</code> to represent the context of that GPU, where <code>N</code> is the number of GPUs in the system. You can run a CUDA graph on a specific GPU by switching the context to a different GPU using <a href="classtf_1_1cudaScopedDevice.html" class="m-doc">tf::<wbr />cudaScopedDevice</a>. The code below creates a CUDA graph and runs it on GPU <code>2</code>.</p><pre class="m-code"><span class="p">{</span>
<span class="w">  </span><span class="c1">// create an RAII-styled switcher to the context of GPU 2</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaScopedDevice</span><span class="w"> </span><span class="nf">context</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// create a CUDA graph under GPU 2</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaGraph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// ...</span>

<span class="w">  </span><span class="c1">// create a stream under GPU 2 and offload the capturer to that GPU</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaStream</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaGraphExec</span><span class="w"> </span><span class="nf">exec</span><span class="p">(</span><span class="n">graph</span><span class="p">);</span>
<span class="w">  </span><span class="n">stream</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">exec</span><span class="p">).</span><span class="n">synchronize</span><span class="p">();</span>
<span class="p">}</span></pre><p><a href="classtf_1_1cudaScopedDevice.html" class="m-doc">tf::<wbr />cudaScopedDevice</a> is an RAII-styled wrapper to perform <em>scoped</em> switch to the given GPU context. When the scope is destroyed, it switches back to the original context.</p><aside class="m-note m-warning"><h4>Attention</h4><p><a href="classtf_1_1cudaScopedDevice.html" class="m-doc">tf::<wbr />cudaScopedDevice</a> allows you to place a CUDA <a href="classtf_1_1Graph.html" class="m-doc">Graph</a> on a particular GPU device, but it is your responsibility to ensure correct memory access. For example, you may not allocate a memory block on GPU <code>2</code> while accessing it from a kernel on GPU <code>0</code>. An easy practice for multi-GPU programming is to allocate <em>unified shared memory</em> using <code>cudaMallocManaged</code> and let the CUDA runtime perform automatic memory migration between GPUs.</p></aside></section><section id="GPUMemoryOperations"><h2><a href="#GPUMemoryOperations">Create Memory Operation Tasks</a></h2><p><a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a> provides a set of methods for users to manipulate device memory. There are two categories, <em>raw</em> data and <em>typed</em> data. Raw data operations are methods with prefix <code>mem</code>, such as <code>memcpy</code> and <code>memset</code>, that operate in <em>bytes</em>. Typed data operations such as <code>copy</code>, <code>fill</code>, and <code>zero</code>, take <em>logical count</em> of elements. For instance, the following three methods have the same result of zeroing <code>sizeof(int)*count</code> bytes of the device memory area pointed to by <code>target</code>.</p><pre class="m-code"><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">target</span><span class="p">;</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

<span class="n">tf</span><span class="o">::</span><span class="n">cudaGraph</span><span class="w"> </span><span class="n">cg</span><span class="p">;</span>
<span class="n">memset_target</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">memset</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">count</span><span class="p">);</span>
<span class="n">same_as_above</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">);</span>
<span class="n">same_as_above_again</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">zero</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">);</span></pre><p>The method <a href="classtf_1_1cudaGraphBase.html#a32634c5645c14b99ceeaafe77ea5ea62" class="m-doc">tf::<wbr />cudaGraph::<wbr />fill</a> is a more powerful variant of <a href="classtf_1_1cudaGraphBase.html#a10196f49de261a4042de328aab2452c8" class="m-doc">tf::<wbr />cudaGraph::<wbr />memset</a>. It can fill a memory area with any value of type <code>T</code>, given that <code>sizeof(T)</code> is 1, 2, or 4 bytes. The following example creates a GPU task to fill <code>count</code> elements in the array <code>target</code> with value <code>1234</code>.</p><pre class="m-code"><span class="n">cf</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="mi">1234</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">);</span></pre><p>Similar concept applies to <a href="classtf_1_1cudaGraphBase.html#a5e704c7bb669a82f4fe140ecb4576eb0" class="m-doc">tf::<wbr />cudaGraph::<wbr />memcpy</a> and <a href="classtf_1_1cudaGraphBase.html#a02a041d5dd9e1e8958eb43e09331051e" class="m-doc">tf::<wbr />cudaGraph::<wbr />copy</a> as well. The following two methods are equivalent to each other.</p><pre class="m-code"><span class="n">cg</span><span class="p">.</span><span class="n">memcpy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">count</span><span class="p">);</span>
<span class="n">cg</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">);</span></pre></section><section id="RunACUDAGraph"><h2><a href="#RunACUDAGraph">Run a CUDA Graph</a></h2><p>To offload a CUDA graph to a GPU, you need to instantiate an executable CUDA graph of <a href="namespacetf.html#a2be50e6880ead1d49a3fec2fc4bb893e" class="m-doc">tf::<wbr />cudaGraphExec</a> and create a <a href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83" class="m-doc">tf::<wbr />cudaStream</a> to run the executable graph. The run method is asynchronous and can be explicitly synchronized on the given stream.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">cudaGraph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="c1">// modify the graph ...</span>

<span class="c1">// create an executable CUDA graph and run it through a stream</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaGraphExec</span><span class="w"> </span><span class="nf">exec</span><span class="p">(</span><span class="n">graph</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaStream</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="n">stream</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">exec</span><span class="p">);</span>

<span class="c1">// wait for the executable cuda graph to finish</span>
<span class="n">stream</span><span class="p">.</span><span class="n">synchronize</span><span class="p">();</span></pre><p>There is always an one-to-one mapping between an <a href="namespacetf.html#a2be50e6880ead1d49a3fec2fc4bb893e" class="m-doc">tf::<wbr />cudaGraphExec</a> and its parent CUDA graph in terms of its graph structure. However, the executable graph is an independent entity and has no lifetime dependency on its parent CUDA graph. You can instantiate multiple executable graphs from the same CUDA graph.</p></section><section id="UpdateAnExecutableCUDAGraph"><h2><a href="#UpdateAnExecutableCUDAGraph">Update an Executable CUDA Graph</a></h2><p>Many GPU applications require launching a CUDA graph multiple times and updating node parameters (e.g., kernel arguments or memory addresses) between iterations. <a href="namespacetf.html#a2be50e6880ead1d49a3fec2fc4bb893e" class="m-doc">tf::<wbr />cudaGraphExec</a> allows you to update the parameters of tasks created from its parent CUDA graph. Every task creation method in <a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a> has a corresponding method in <a href="namespacetf.html#a2be50e6880ead1d49a3fec2fc4bb893e" class="m-doc">tf::<wbr />cudaGraphExec</a> for updating the parameters of that task.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">cudaStream</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaGraph</span><span class="w"> </span><span class="n">cg</span><span class="p">;</span>

<span class="c1">// create a kernel task</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">grid1</span><span class="p">,</span><span class="w"> </span><span class="n">block1</span><span class="p">,</span><span class="w"> </span><span class="n">shm1</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">kernel_args_1</span><span class="p">);</span>

<span class="c1">// instantiate an executable graph</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaGraphExec</span><span class="w"> </span><span class="nf">exec</span><span class="p">(</span><span class="n">cg</span><span class="p">);</span>
<span class="n">stream</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">stream</span><span class="p">).</span><span class="n">synchronize</span><span class="p">();</span>

<span class="c1">// update the created kernel task with different parameters</span>
<span class="n">exec</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">grid2</span><span class="p">,</span><span class="w"> </span><span class="n">block2</span><span class="p">,</span><span class="w"> </span><span class="n">shm2</span><span class="p">,</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">kernel_args_2</span><span class="p">);</span>

<span class="c1">// run the updated executable graph</span>
<span class="n">stream</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">stream</span><span class="p">).</span><span class="n">synchronize</span><span class="p">();</span></pre><p>Between successive offloads (i.e., iterative executions of a CUDA graph), you can <em>ONLY</em> update task parameters, such as changing the kernel execution parameters and memory operation parameters. However, you must <em>NOT</em> change the topology of the CUDA graph, such as adding a new task or adding a new dependency. This is the limitation of Nvidia CUDA Graph.</p><aside class="m-note m-warning"><h4>Attention</h4><p>There are a few restrictions on updating task parameters in an executable CUDA graph:</p><ul><li>You cannot change a task to a different type</li><li>kernel task<ul><li>The kernel function is not allowed to change. This restriction applies to all algorithm tasks that are created using lambda.</li></ul></li><li>memset and memcpy tasks:<ul><li>The CUDA device(s) to which the operand(s) was allocated/mapped cannot change</li><li>The source/destination memory must be allocated from the same contexts as the original source/destination memory.</li></ul></li></ul></aside></section><section id="IntegrateACUDAGraphIntoTaskflow"><h2><a href="#IntegrateACUDAGraphIntoTaskflow">Integrate a CUDA Graph into Taskflow</a></h2><p>As <a href="namespacetf.html#a713c427e4f9841a90dec67045a3babed" class="m-doc">tf::<wbr />cudaGraph</a> is a standalone wrapper over Nvidia CUDA Graph, you can simply run it as a task. The following example runs a CUDA graph from a static task:</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">Executor</span><span class="w"> </span><span class="n">executor</span><span class="p">;</span>
<span class="n">tf</span><span class="o">::</span><span class="n">Taskflow</span><span class="w"> </span><span class="n">taskflow</span><span class="p">;</span>

<span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([](){</span>
<span class="w">  </span><span class="c1">// create a CUDA graph inside a static task</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaGraph</span><span class="w"> </span><span class="n">cg</span><span class="p">;</span>
<span class="w">  </span><span class="n">cg</span><span class="p">.</span><span class="n">kernel</span><span class="p">(...);</span>
<span class="w">  </span>
<span class="w">  </span><span class="c1">// instantiate a CUDA graph executable and run it through a stream</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaGraphExec</span><span class="w"> </span><span class="nf">ecec</span><span class="p">(</span><span class="n">cg</span><span class="p">);</span>
<span class="w">  </span><span class="n">tf</span><span class="o">::</span><span class="n">cudaStream</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="w">  </span><span class="n">stream</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">exec</span><span class="p">).</span><span class="n">synchronize</span><span class="p">();</span>
<span class="p">});</span></pre></section>
      </div>
    </div>
  </div>
</article></main>
<div class="m-doc-search" id="search">
  <a href="#!" onclick="return hideSearch()"></a>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-m-8 m-push-m-2">
        <div class="m-doc-search-header m-text m-small">
          <div><span class="m-label m-default">Tab</span> / <span class="m-label m-default">T</span> to search, <span class="m-label m-default">Esc</span> to close</div>
          <div id="search-symbolcount">&hellip;</div>
        </div>
        <div class="m-doc-search-content">
          <form>
            <input type="search" name="q" id="search-input" placeholder="Loading &hellip;" disabled="disabled" autofocus="autofocus" autocomplete="off" spellcheck="false" />
          </form>
          <noscript class="m-text m-danger m-text-center">Unlike everything else in the docs, the search functionality <em>requires</em> JavaScript.</noscript>
          <div id="search-help" class="m-text m-dim m-text-center">
            <p class="m-noindent">Search for symbols, directories, files, pages or
            modules. You can omit any prefix from the symbol or file path; adding a
            <code>:</code> or <code>/</code> suffix lists all members of given symbol or
            directory.</p>
            <p class="m-noindent">Use <span class="m-label m-dim">&darr;</span>
            / <span class="m-label m-dim">&uarr;</span> to navigate through the list,
            <span class="m-label m-dim">Enter</span> to go.
            <span class="m-label m-dim">Tab</span> autocompletes common prefix, you can
            copy a link to the result using <span class="m-label m-dim">⌘</span>
            <span class="m-label m-dim">L</span> while <span class="m-label m-dim">⌘</span>
            <span class="m-label m-dim">M</span> produces a Markdown link.</p>
          </div>
          <div id="search-notfound" class="m-text m-warning m-text-center">Sorry, nothing was found.</div>
          <ul id="search-results"></ul>
        </div>
      </div>
    </div>
  </div>
</div>
<script src="search-v2.js"></script>
<script src="searchdata-v2.js" async="async"></script>
<footer><nav>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <p>Taskflow handbook is part of the <a href="https://taskflow.github.io">Taskflow project</a>, copyright © <a href="https://tsung-wei-huang.github.io/">Dr. Tsung-Wei Huang</a>, 2018&ndash;2025.<br />Generated by <a href="https://doxygen.org/">Doxygen</a> 1.12.0 and <a href="https://mcss.mosra.cz/">m.css</a>.</p>
      </div>
    </div>
  </div>
</nav></footer>
</body>
</html>
