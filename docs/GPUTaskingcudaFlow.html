<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Cookbook &raquo; GPU Tasking (cudaFlow) | Taskflow QuickStart</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,600i%7CSource+Code+Pro:400,400i,600" />
  <link rel="stylesheet" href="m-dark+documentation.compiled.css" />
  <link rel="icon" href="favicon.ico" type="image/x-icon" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#22272e" />
</head>
<body>
<header><nav id="navigation">
  <div class="m-container">
    <div class="m-row">
      <span id="m-navbar-brand" class="m-col-t-8 m-col-m-none m-left-m">
        <a href="https://taskflow.github.io"><img src="taskflow_logo.png" alt="" />Taskflow</a> <span class="m-breadcrumb">|</span> <a href="index.html" class="m-thin">QuickStart</a>
      </span>
      <div class="m-col-t-4 m-hide-m m-text-right m-nopadr">
        <a href="#search" class="m-doc-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
          <path id="m-doc-search-icon-path" d="m6 0c-3.31 0-6 2.69-6 6 0 3.31 2.69 6 6 6 1.49 0 2.85-0.541 3.89-1.44-0.0164 0.338 0.147 0.759 0.5 1.15l3.22 3.79c0.552 0.614 1.45 0.665 2 0.115 0.55-0.55 0.499-1.45-0.115-2l-3.79-3.22c-0.392-0.353-0.812-0.515-1.15-0.5 0.895-1.05 1.44-2.41 1.44-3.89 0-3.31-2.69-6-6-6zm0 1.56a4.44 4.44 0 0 1 4.44 4.44 4.44 4.44 0 0 1-4.44 4.44 4.44 4.44 0 0 1-4.44-4.44 4.44 4.44 0 0 1 4.44-4.44z"/>
        </svg></a>
        <a id="m-navbar-show" href="#navigation" title="Show navigation"></a>
        <a id="m-navbar-hide" href="#" title="Hide navigation"></a>
      </div>
      <div id="m-navbar-collapse" class="m-col-t-12 m-show-m m-col-m-none m-right-m">
        <div class="m-row">
          <ol class="m-col-t-6 m-col-m-none">
            <li><a href="pages.html">Handbook</a></li>
            <li><a href="namespaces.html">Namespaces</a></li>
          </ol>
          <ol class="m-col-t-6 m-col-m-none" start="3">
            <li><a href="annotated.html">Classes</a></li>
            <li><a href="files.html">Files</a></li>
            <li class="m-show-m"><a href="#search" class="m-doc-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
              <use href="#m-doc-search-icon-path" />
            </svg></a></li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</nav></header>
<main><article>
  <div class="m-container m-container-inflatable">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <h1>
          <span class="m-breadcrumb"><a href="Cookbook.html">Cookbook</a> &raquo;</span>
          GPU Tasking (cudaFlow)
        </h1>
        <div class="m-block m-default">
          <h3>Contents</h3>
          <ul>
            <li><a href="#Create_a_cudaFlow">Create a cudaFlow</a></li>
            <li><a href="#Compile_a_cudaFlow_program">Compile a cudaFlow Program</a></li>
            <li><a href="#run_a_cudaflow_on_multiple_gpus">Run a cudaFlow on Multiple GPUs</a></li>
            <li><a href="#GPUMemoryOperations">Access GPU Memory</a></li>
            <li><a href="#StudyThecudaFlowGranularity">Study the Granularity</a></li>
            <li><a href="#OffloadAndUpdateAcudaFlow">Offload and Update a cudaFlow</a></li>
            <li><a href="#UsecudaFlowInAStandaloneEnvironment">Use cudaFlow in a Standalone Environment</a></li>
          </ul>
        </div>
<p>Modern scientific computing typically leverages GPU-powered parallel processing cores to speed up large-scale applications. This chapter discusses how to implement CPU-GPU heterogeneous tasking algorithms with <a href="https://developer.nvidia.com/cuda-zone">Nvidia CUDA</a>.</p><section id="Create_a_cudaFlow"><h2><a href="#Create_a_cudaFlow">Create a cudaFlow</a></h2><p>Taskflow enables concurrent CPU-GPU tasking by leveraging <a href="https://developer.nvidia.com/blog/cuda-graphs/">CUDA Graph</a>. The tasking interface is referred to as <em>cudaFlow</em>. A cudaFlow is a graph object of type <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> created at runtime similar to dynamic tasking. It manages a task node in a taskflow and associates it with a CUDA Graph. To create a cudaFlow, emplace a callable with an argument of type <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a>. The following example implements the canonical saxpy (A·X Plus Y) task graph.</p><pre class="m-code"> <span class="mi">1</span><span class="o">:</span> <span class="err">#</span><span class="n">include</span> <span class="o">&lt;</span><span class="n">taskflow</span><span class="o">/</span><span class="n">cudaflow</span><span class="p">.</span><span class="n">hpp</span><span class="o">&gt;</span>
 <span class="mi">2</span><span class="o">:</span> 
 <span class="mi">3</span><span class="o">:</span> <span class="c1">// saxpy (single-precision A·X Plus Y) kernel</span>
 <span class="mi">4</span><span class="o">:</span> <span class="n">__global__</span> <span class="kt">void</span> <span class="n">saxpy</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">5</span><span class="o">:</span>   <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
 <span class="mi">6</span><span class="o">:</span>   <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">7</span><span class="o">:</span>     <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
 <span class="mi">8</span><span class="o">:</span>   <span class="p">}</span>
 <span class="mi">9</span><span class="o">:</span> <span class="p">}</span>
<span class="mi">10</span><span class="o">:</span>
<span class="mi">11</span><span class="o">:</span> <span class="c1">// main function begins</span>
<span class="mi">12</span><span class="o">:</span> <span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
<span class="mi">13</span><span class="o">:</span>
<span class="mi">14</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Taskflow</span> <span class="n">taskflow</span><span class="p">;</span>
<span class="mi">15</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Executor</span> <span class="n">executor</span><span class="p">;</span>
<span class="mi">16</span><span class="o">:</span>  
<span class="mi">17</span><span class="o">:</span>   <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span>                            <span class="c1">// size of the vector</span>
<span class="mi">18</span><span class="o">:</span>
<span class="mi">19</span><span class="o">:</span>   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">hx</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">1.0f</span><span class="p">);</span>                      <span class="c1">// x vector at host</span>
<span class="mi">20</span><span class="o">:</span>   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">hy</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">);</span>                      <span class="c1">// y vector at host</span>
<span class="mi">21</span><span class="o">:</span>
<span class="mi">22</span><span class="o">:</span>   <span class="kt">float</span> <span class="o">*</span><span class="n">dx</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>                                  <span class="c1">// x vector at device</span>
<span class="mi">23</span><span class="o">:</span>   <span class="kt">float</span> <span class="o">*</span><span class="n">dy</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>                                  <span class="c1">// y vector at device</span>
<span class="mi">24</span><span class="o">:</span>  
<span class="mi">25</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">allocate_x</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span>
<span class="mi">26</span><span class="o">:</span>     <span class="p">[</span><span class="o">&amp;</span><span class="p">](){</span> <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));}</span>
<span class="mi">27</span><span class="o">:</span>   <span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;allocate_x&quot;</span><span class="p">);</span>
<span class="mi">28</span><span class="o">:</span>
<span class="mi">29</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">allocate_y</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span>
<span class="mi">30</span><span class="o">:</span>     <span class="p">[</span><span class="o">&amp;</span><span class="p">](){</span> <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));}</span>
<span class="mi">31</span><span class="o">:</span>   <span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;allocate_y&quot;</span><span class="p">);</span>
<span class="mi">32</span><span class="o">:</span>
<span class="mi">33</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaflow</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">34</span><span class="o">:</span>     <span class="c1">// create data transfer tasks</span>
<span class="mi">35</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span> 
<span class="mi">36</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
<span class="mi">37</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
<span class="mi">38</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
<span class="mi">39</span><span class="o">:</span>
<span class="mi">40</span><span class="o">:</span>     <span class="c1">// launch saxpy&lt;&lt;&lt;(N+255)/256, 256, 0&gt;&gt;&gt;(N, 2.0f, dx, dy)</span>
<span class="mi">41</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">kernel</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span>
<span class="mi">42</span><span class="o">:</span>       <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
<span class="mi">43</span><span class="o">:</span>     <span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
<span class="mi">44</span><span class="o">:</span>
<span class="mi">45</span><span class="o">:</span>     <span class="n">kernel</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>
<span class="mi">46</span><span class="o">:</span>           <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span>
<span class="mi">48</span><span class="o">:</span>   <span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
<span class="mi">49</span><span class="o">:</span>   <span class="n">cudaflow</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">allocate_x</span><span class="p">,</span> <span class="n">allocate_y</span><span class="p">);</span>  <span class="c1">// overlap memory alloc</span>
<span class="mi">50</span><span class="o">:</span>  
<span class="mi">51</span><span class="o">:</span>   <span class="n">executor</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">taskflow</span><span class="p">).</span><span class="n">wait</span><span class="p">();</span>
<span class="mi">52</span><span class="o">:</span>
<span class="mi">53</span><span class="o">:</span>   <span class="n">taskflow</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="p">);</span>                  <span class="c1">// dump the taskflow</span>
<span class="mi">54</span><span class="o">:</span> <span class="p">}</span></pre><div class="m-graph"><svg style="width: 30.000rem; height: 14.562rem;" viewBox="0.00 0.00 479.55 232.77">
<g transform="scale(1 1) rotate(0) translate(4 228.77)">
<title>Taskflow</title>
<g class="m-cluster">
<title>cluster_p0x55b2191178a8</title>
<polygon points="8,-47.38 8,-180.38 463.55,-180.38 463.55,-47.38 8,-47.38"/>
<text text-anchor="middle" x="235.77" y="-163.58">cudaFlow: saxpy</text>
</g>
<g class="m-node m-flat">
<title>p0x55b219117698</title>
<ellipse cx="296.91" cy="-206.38" rx="63.78" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-202.58">allocate_x</text>
</g>
<g class="m-node">
<title>p0x55b2191178a8</title>
<polygon points="455.55,-118.38 452.55,-122.38 431.55,-122.38 428.55,-118.38 396.55,-118.38 396.55,-82.38 455.55,-82.38 455.55,-118.38"/>
<text text-anchor="middle" x="426.05" y="-96.58">saxpy</text>
</g>
<g class="m-edge">
<title>p0x55b219117698&#45;&gt;p0x55b2191178a8</title>
<path d="M343.62,-193.87C349.59,-191.24 355.41,-188.1 360.55,-184.38 381.31,-169.35 398.59,-145.77 410.02,-127.42"/>
<polygon points="413.15,-129 415.3,-118.63 407.15,-125.4 413.15,-129"/>
</g>
<g class="m-node m-flat">
<title>p0x55b2191177a0</title>
<ellipse cx="296.91" cy="-18.38" rx="63.78" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-14.58">allocate_y</text>
</g>
<g class="m-edge">
<title>p0x55b2191177a0&#45;&gt;p0x55b2191178a8</title>
<path d="M338.66,-32.28C346.17,-35.49 353.77,-39.21 360.55,-43.38 374.92,-52.23 389.16,-64.37 400.6,-75.17"/>
<polygon points="398.41,-77.92 408.03,-82.37 403.29,-72.9 398.41,-77.92"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870401a50</title>
<ellipse cx="59.13" cy="-128.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="59.13" y="-124.58">h2d_x</text>
</g>
<g class="m-node">
<title>p0x7f2870402bc0</title>
<polygon points="197.27,-118.38 142.27,-118.38 138.27,-114.38 138.27,-82.38 193.27,-82.38 197.27,-86.38 197.27,-118.38"/>
<polyline points="193.27,-114.38 138.27,-114.38 "/>
<polyline points="193.27,-114.38 193.27,-82.38 "/>
<polyline points="193.27,-114.38 197.27,-118.38 "/>
<text text-anchor="middle" x="167.77" y="-96.58">saxpy</text>
</g>
<g class="m-edge">
<title>p0x7f2870401a50&#45;&gt;p0x7f2870402bc0</title>
<path d="M96.23,-118.91C106.58,-116.19 117.9,-113.22 128.38,-110.46"/>
<polygon points="129.33,-113.83 138.11,-107.91 127.55,-107.06 129.33,-113.83"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402310</title>
<ellipse cx="296.91" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-69.58">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402310</title>
<path d="M197.3,-94.33C212.23,-91.15 230.92,-87.19 247.99,-83.56"/>
<polygon points="248.77,-86.97 257.82,-81.47 247.31,-80.13 248.77,-86.97"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402780</title>
<ellipse cx="296.91" cy="-128.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-124.58">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402780</title>
<path d="M197.3,-106.67C212.36,-109.98 231.22,-114.14 248.41,-117.93"/>
<polygon points="247.78,-121.37 258.3,-120.1 249.29,-114.53 247.78,-121.37"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870401eb0</title>
<ellipse cx="59.13" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="59.13" y="-69.58">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870401eb0&#45;&gt;p0x7f2870402bc0</title>
<path d="M96.53,-82.6C106.67,-85.17 117.71,-87.96 127.98,-90.56"/>
<polygon points="127.37,-94.02 137.93,-93.08 129.09,-87.24 127.37,-94.02"/>
</g>
<g class="m-edge">
<title>p0x7f2870402310&#45;&gt;p0x55b2191178a8</title>
<path d="M336.05,-81.49C351.89,-84.85 370.25,-88.75 386.17,-92.13"/>
<polygon points="385.72,-95.61 396.23,-94.26 387.17,-88.76 385.72,-95.61"/>
</g>
<g class="m-edge">
<title>p0x7f2870402780&#45;&gt;p0x55b2191178a8</title>
<path d="M335.71,-120.06C351.68,-116.54 370.26,-112.45 386.33,-108.91"/>
<polygon points="387.46,-112.25 396.48,-106.68 385.96,-105.41 387.46,-112.25"/>
</g>
</g>
</svg>
</div><p>Debrief:</p><ul><li>Lines 3-9 define a saxpy kernel using CUDA</li><li>Lines 19-20 declare two host vectors, <code>hx</code> and <code>hy</code></li><li>Lines 22-23 declare two device vector pointers, <code>dx</code> and <code>dy</code></li><li>Lines 25-31 declare two tasks to allocate memory for <code>dx</code> and <code>dy</code> on device, each of <code>N*sizeof(float)</code> bytes</li><li>Lines 33-48 create a cudaFlow to define a GPU task graph (two host-to-device data transfer tasks, one saxpy kernel task, and two device-to-host data transfer tasks)</li><li>Lines 49-53 define the task dependency between host tasks and the cudaFlow tasks and execute the taskflow</li></ul><p>Taskflow does not expend unnecessary efforts on kernel programming but focus on tasking CUDA operations with CPU work. We give users full privileges to craft a CUDA kernel that is commensurate with their domain knowledge. Users focus on developing high-performance kernels using a native CUDA toolkit, while leaving difficult task parallelism to Taskflow.</p><aside class="m-note m-warning"><h4>Attention</h4><p>You need to include <code><a href="cudaflow_8hpp.html" class="m-doc">taskflow/<wbr />cudaflow.hpp</a></code> in order to use <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a>.</p></aside></section><section id="Compile_a_cudaFlow_program"><h2><a href="#Compile_a_cudaFlow_program">Compile a cudaFlow Program</a></h2><p>Use <a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">nvcc</a> (at least v11.1) to compile a cudaFlow program:</p><pre class="m-console"><span class="go">~$ nvcc -std=c++17 my_cudaflow.cu -I path/to/include/taskflow -O2 -o my_cudaflow</span>
<span class="go">~$ ./my_cudaflow</span></pre><p>Please visit the page <a href="CompileTaskflowWithCUDA.html" class="m-doc">Compile Taskflow with CUDA</a> for more details.</p></section><section id="run_a_cudaflow_on_multiple_gpus"><h2><a href="#run_a_cudaflow_on_multiple_gpus">Run a cudaFlow on Multiple GPUs</a></h2><p>By default, a cudaFlow runs on the current GPU associated with the caller, which is typically <code>0</code>. You can run a cudaFlow on multiple GPUs by explicitly associating a cudaFlow or a kernel task with a CUDA device. A CUDA device is an integer number in the range of <code>[0, N)</code> representing the identifier of a GPU, where <code>N</code> is the number of GPUs in a system. The code below creates a cudaFlow that runs on GPU <code>0</code>.</p><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([]</span> <span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{},</span> <span class="mi">0</span><span class="p">);</span>  <span class="c1">// place the cudaFlow on GPU 0</span></pre><p>You can place a kernel on a GPU explicitly through the method <a href="classtf_1_1cudaFlow.html#a4a839dbaa01237a440edfebe8faf4e5b" class="m-doc">tf::<wbr />cudaFlow::<wbr />kernel_on</a> that takes the GPU device identifier in the first argument.</p><pre class="m-code"> <span class="mi">1</span><span class="o">:</span> <span class="err">#</span><span class="n">include</span> <span class="o">&lt;</span><span class="n">taskflow</span><span class="o">/</span><span class="n">cudaflow</span><span class="p">.</span><span class="n">hpp</span><span class="o">&gt;</span>
 <span class="mi">2</span><span class="o">:</span> 
 <span class="mi">3</span><span class="o">:</span> <span class="c1">// saxpy (single-precision A·X Plus Y) kernel</span>
 <span class="mi">4</span><span class="o">:</span> <span class="n">__global__</span> <span class="kt">void</span> <span class="n">saxpy</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">z</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">5</span><span class="o">:</span>  <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
 <span class="mi">6</span><span class="o">:</span>  <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">7</span><span class="o">:</span>    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
 <span class="mi">8</span><span class="o">:</span>   <span class="p">}</span>
 <span class="mi">9</span><span class="o">:</span> <span class="p">}</span>
<span class="mi">10</span><span class="o">:</span>
<span class="mi">11</span><span class="o">:</span> <span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
<span class="mi">12</span><span class="o">:</span>
<span class="mi">13</span><span class="o">:</span>   <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span>
<span class="mi">14</span><span class="o">:</span>   
<span class="mi">15</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">dx</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">16</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">dy</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">17</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">z1</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">18</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">z2</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">19</span><span class="o">:</span>  
<span class="mi">20</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// create unified memory for x</span>
<span class="mi">21</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// create unified memory for y</span>
<span class="mi">22</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">z1</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// result of saxpy task 1</span>
<span class="mi">23</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">z2</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// result of saxpy task 2</span>
<span class="mi">24</span><span class="o">:</span>  
<span class="mi">25</span><span class="o">:</span>   <span class="k">for</span><span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">26</span><span class="o">:</span>     <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="mi">27</span><span class="o">:</span>     <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="mi">28</span><span class="o">:</span>   <span class="p">}</span>
<span class="mi">29</span><span class="o">:</span>
<span class="mi">30</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Taskflow</span> <span class="n">taskflow</span><span class="p">;</span>
<span class="mi">31</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Executor</span> <span class="n">executor</span><span class="p">;</span>
<span class="mi">32</span><span class="o">:</span>  
<span class="mi">33</span><span class="o">:</span>   <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
<span class="mi">34</span><span class="o">:</span>     <span class="c1">// We create a cudaFlow on GPU 0. The scheduler will switch to </span>
<span class="mi">35</span><span class="o">:</span>     <span class="c1">// GPU context 0 when running this callable.</span>
<span class="mi">36</span><span class="o">:</span>
<span class="mi">37</span><span class="o">:</span>     <span class="c1">// launch the first saxpy kernel on GPU 1</span>
<span class="mi">38</span><span class="o">:</span>     <span class="n">cf</span><span class="p">.</span><span class="n">kernel_on</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z1</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;1&quot;</span><span class="p">);</span>
<span class="mi">39</span><span class="o">:</span>
<span class="mi">40</span><span class="o">:</span>     <span class="c1">// launch the second saxpy kernel on GPU 3</span>
<span class="mi">41</span><span class="o">:</span>     <span class="n">cf</span><span class="p">.</span><span class="n">kernel_on</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z2</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;3&quot;</span><span class="p">);</span>
<span class="mi">42</span><span class="o">:</span>   <span class="p">},</span> <span class="mi">0</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;cudaFlow on GPU 0&quot;</span><span class="p">);</span>
<span class="mi">43</span><span class="o">:</span>
<span class="mi">44</span><span class="o">:</span>   <span class="n">executor</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">taskflow</span><span class="p">).</span><span class="n">wait</span><span class="p">();</span>
<span class="mi">45</span><span class="o">:</span>
<span class="mi">46</span><span class="o">:</span>   <span class="n">cudaFree</span><span class="p">(</span><span class="n">dx</span><span class="p">);</span>
<span class="mi">47</span><span class="o">:</span>   <span class="n">cudaFree</span><span class="p">(</span><span class="n">dy</span><span class="p">);</span>
<span class="mi">48</span><span class="o">:</span>  
<span class="mi">49</span><span class="o">:</span>   <span class="c1">// verify the solution; max_error should be zero</span>
<span class="mi">50</span><span class="o">:</span>   <span class="kt">float</span> <span class="n">max_error</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="mi">51</span><span class="o">:</span>   <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">52</span><span class="o">:</span>     <span class="n">max_error</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">max_error</span><span class="p">,</span> <span class="n">abs</span><span class="p">(</span><span class="n">z1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="mi">-4</span><span class="p">));</span>
<span class="mi">53</span><span class="o">:</span>     <span class="n">max_error</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">max_error</span><span class="p">,</span> <span class="n">abs</span><span class="p">(</span><span class="n">z2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="mi">-4</span><span class="p">));</span>
<span class="mi">54</span><span class="o">:</span>   <span class="p">}</span>
<span class="mi">55</span><span class="o">:</span>   <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;saxpy finished with max error: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">max_error</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;\n&#39;</span><span class="p">;</span>
<span class="mi">56</span><span class="o">:</span> <span class="p">}</span></pre><p>Debrief:</p><ul><li>Lines 3-9 define a CUDA saxpy kernel that stores the result to <code>z</code></li><li>Lines 15-23 declare four unified memory blocks accessible from any processor</li><li>Lines 25-28 initialize <code>dx</code> and <code>dy</code> blocks by CPU</li><li>Lines 33-42 create a cudaFlow task on GPU <code>0</code> using <a href="classtf_1_1FlowBuilder.html#afdf47fd1a358fb64f8c1b89e2a393169" class="m-doc">tf::<wbr />Taskflow::<wbr />emplace_on</a></li><li>Lines 37-38 create a kernel task to launch the first saxpy on GPU <code>1</code> and store the result in <code>z1</code></li><li>Lines 40-41 create a kernel task to launch the second saxpy on GPU <code>3</code> and store the result in <code>z2</code></li><li>Lines 44-55 run the taskflow and verify the result (<code>max_error</code> should be zero)</li></ul><p>Running the program gives the following <a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">nvidia-smi</a> snapshot in a system of 4 GPUs:</p><pre class="m-console"><span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                       GPU Memory |</span>
<span class="go">|  GPU       PID   Type   Process name                             Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|    0     53869      C   ./a.out                                      153MiB |</span>
<span class="go">|    1     53869      C   ./a.out                                      155MiB |</span>
<span class="go">|    3     53869      C   ./a.out                                      155MiB |</span>
<span class="go">+-----------------------------------------------------------------------------+</span></pre><aside class="m-note m-warning"><h4>Attention</h4><p><a href="classtf_1_1FlowBuilder.html#afdf47fd1a358fb64f8c1b89e2a393169" class="m-doc">tf::<wbr />Taskflow::<wbr />emplace_on</a> allows you to place a cudaFlow on a particular GPU device, but it is your responsibility to ensure correct memory access. For example, you may not allocate a memory block on GPU <code>2</code> using <code>cudaMalloc</code> and access it from a kernel on GPU <code>1</code>.</p></aside><p>An easy practice is to allocate <em>unified shared memory</em> using <code>cudaMallocManaged</code> and let the CUDA runtime perform automatic memory migration between processors (as demonstrated in the code example above).</p><p>As the same example, you may create two cudaFlows for the two kernels on two GPUs, respectively. The overhead of creating a kernel on the same device as a cudaFlow is much less than the different one.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaFlow_on_gpu1</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z1</span><span class="p">);</span>
<span class="p">},</span> <span class="mi">1</span><span class="p">);</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaFlow_on_gpu3</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z2</span><span class="p">);</span>
<span class="p">},</span> <span class="mi">3</span><span class="p">);</span></pre></section><section id="GPUMemoryOperations"><h2><a href="#GPUMemoryOperations">Access GPU Memory</a></h2><p><a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> provides a set of methods for users to manipulate device memory data. There are two categories, raw data and typed data. Raw data operations are methods with prefix <code>mem</code>, such as <code>memcpy</code> and <code>memset</code>, that take action on GPU memory area in <em>bytes</em>. Typed data operations such as <code>copy</code>, <code>fill</code>, and <code>zero</code>, take <em>logical count</em> of elements. For instance, the following three methods have the same result of zeroing <code>sizeof(int)*count</code> bytes of the device memory area pointed to by <code>target</code>.</p><pre class="m-code"><span class="kt">int</span><span class="o">*</span> <span class="n">target</span><span class="p">;</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">target</span><span class="p">,</span> <span class="n">count</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

<span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">memset_target</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">memset</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">count</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">same_as_above</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">same_as_above_again</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">zero</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
<span class="p">});</span></pre><p>The method <a href="classtf_1_1cudaFlow.html#a21d4447bc834f4d3e1bb4772c850d090" class="m-doc">cudaFlow::<wbr />fill</a> is a more powerful version of <a href="classtf_1_1cudaFlow.html#a079ca65da35301e5aafd45878a19e9d2" class="m-doc">cudaFlow::<wbr />memset</a>. It can fill a memory area with any value of type <code>T</code>, given that <code>sizeof(T)</code> is 1, 2, or 4 bytes. For example, the following code sets each element in the array <code>target</code> to 1234.</p><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span> <span class="n">cf</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span> <span class="p">});</span></pre><p>Similar concept applies to <a href="classtf_1_1cudaFlow.html#ad37637606f0643f360e9eda1f9a6e559" class="m-doc">cudaFlow::<wbr />memcpy</a> and <a href="classtf_1_1cudaFlow.html#af03e04771b655f9e629eb4c22e19b19f" class="m-doc">cudaFlow::<wbr />copy</a> as well.</p><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">memcpy_target</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">memcpy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">count</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">same_as_above</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
<span class="p">});</span></pre></section><section id="StudyThecudaFlowGranularity"><h2><a href="#StudyThecudaFlowGranularity">Study the Granularity</a></h2><p>Creating a cudaFlow has certain overhead, which means fined-grained tasking such as one GPU operation per cudaFlow may not give you any performance gain. You should aggregate as many GPU operations as possible in a cudaFlow to launch the entire graph once instead of separate calls. For example, the following code creates the saxpy task graph at a very fine-grained level using one cudaFlow per GPU operation.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>  <span class="c1">// creates the 1st cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>  <span class="c1">// creates the 2nd cudaFlow </span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>  <span class="c1">// creates the 3rd cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>  <span class="c1">// creates the 4th cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">kernel</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;kernel&quot;</span><span class="p">);</span> <span class="c1">// creates the 5th cudaFlow</span>

<span class="n">kernel</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>
      <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span></pre><div class="m-graph"><svg style="width: 37.750rem; height: 21.250rem;" viewBox="0.00 0.00 604.00 339.54">
<g transform="scale(1 1) rotate(0) translate(4 335.54)">
<title>Taskflow</title>
<g class="m-cluster">
<title>cluster_p0x21987b0</title>
<polygon points="449,-166.77 449,-323.54 588,-323.54 588,-166.77 449,-166.77"/>
<text text-anchor="middle" x="518.5" y="-306.74">cudaFlow: h2d_x</text>
</g>
<g class="m-cluster">
<title>cluster_p0x2198870</title>
<polygon points="302,-166.77 302,-323.54 441,-323.54 441,-166.77 302,-166.77"/>
<text text-anchor="middle" x="371.5" y="-306.74">cudaFlow: h2d_y</text>
</g>
<g class="m-cluster">
<title>cluster_p0x2198930</title>
<polygon points="8,-8 8,-158.77 147,-158.77 147,-8 8,-8"/>
<text text-anchor="middle" x="77.5" y="-141.97">cudaFlow: d2h_x</text>
</g>
<g class="m-cluster">
<title>cluster_p0x21989f0</title>
<polygon points="302,-8 302,-158.77 441,-158.77 441,-8 302,-8"/>
<text text-anchor="middle" x="371.5" y="-141.97">cudaFlow: d2h_y</text>
</g>
<g class="m-cluster">
<title>cluster_p0x2198ab0</title>
<polygon points="155,-80.38 155,-244.77 294,-244.77 294,-80.38 155,-80.38"/>
<text text-anchor="middle" x="224.5" y="-227.97">cudaFlow: kernel</text>
</g>
<g class="m-node">
<title>p0x21987b0</title>
<polygon points="524.5,-210.77 521.5,-214.77 500.5,-214.77 497.5,-210.77 463.5,-210.77 463.5,-174.77 524.5,-174.77 524.5,-210.77"/>
<text text-anchor="middle" x="494" y="-188.97">h2d_x</text>
</g>
<g class="m-node">
<title>p0x2198ab0</title>
<polygon points="285.5,-124.38 282.5,-128.38 261.5,-128.38 258.5,-124.38 224.5,-124.38 224.5,-88.38 285.5,-88.38 285.5,-124.38"/>
<text text-anchor="middle" x="255" y="-102.58">kernel</text>
</g>
<g class="m-edge">
<title>p0x21987b0&#45;&gt;p0x2198ab0</title>
<path d="M465.52,-174.74C458.99,-171.55 451.92,-168.63 445,-166.77 413.41,-158.26 327.25,-173.42 298,-158.77 286.58,-153.05 277.04,-142.78 269.87,-132.92"/>
<polygon points="272.68,-130.82 264.2,-124.48 266.88,-134.73 272.68,-130.82"/>
</g>
<g class="m-node">
<title>p0x2198930</title>
<polygon points="132.5,-52 129.5,-56 108.5,-56 105.5,-52 71.5,-52 71.5,-16 132.5,-16 132.5,-52"/>
<text text-anchor="middle" x="102" y="-30.2">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x2198ab0&#45;&gt;p0x2198930</title>
<path d="M224.41,-91.31C200.64,-80.38 167.49,-65.13 141.69,-53.26"/>
<polygon points="143.09,-50.05 132.54,-49.05 140.16,-56.41 143.09,-50.05"/>
</g>
<g class="m-node">
<title>p0x21989f0</title>
<polygon points="377.5,-52 374.5,-56 353.5,-56 350.5,-52 316.5,-52 316.5,-16 377.5,-16 377.5,-52"/>
<text text-anchor="middle" x="347" y="-30.2">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x2198ab0&#45;&gt;p0x21989f0</title>
<path d="M277.27,-88.35C289.05,-79.34 303.69,-68.14 316.5,-58.33"/>
<polygon points="318.8,-60.98 324.62,-52.13 314.55,-55.42 318.8,-60.98"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe390000e60</title>
<ellipse cx="500" cy="-271.15" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="500" y="-267.35">h2d_x</text>
</g>
<g class="m-edge">
<title>p0x7fe390000e60&#45;&gt;p0x21987b0</title>
<path d="M498.61,-252.44C497.88,-243.15 496.97,-231.56 496.15,-221.15"/>
<polygon points="499.62,-220.56 495.34,-210.86 492.64,-221.11 499.62,-220.56"/>
</g>
<g class="m-node">
<title>p0x2198870</title>
<polygon points="377.5,-210.77 374.5,-214.77 353.5,-214.77 350.5,-210.77 316.5,-210.77 316.5,-174.77 377.5,-174.77 377.5,-210.77"/>
<text text-anchor="middle" x="347" y="-188.97">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x2198870&#45;&gt;p0x2198ab0</title>
<path d="M318.78,-174.73C311.7,-169.94 304.32,-164.47 298,-158.77 289.36,-150.98 280.9,-141.43 273.81,-132.69"/>
<polygon points="276.4,-130.33 267.46,-124.64 270.9,-134.66 276.4,-130.33"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe390001890</title>
<ellipse cx="353" cy="-271.15" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="353" y="-267.35">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x7fe390001890&#45;&gt;p0x2198870</title>
<path d="M351.61,-252.44C350.88,-243.15 349.97,-231.56 349.15,-221.15"/>
<polygon points="352.62,-220.56 348.34,-210.86 345.64,-221.11 352.62,-220.56"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe39000b790</title>
<ellipse cx="96" cy="-106.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="96" y="-102.58">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x7fe39000b790&#45;&gt;p0x2198930</title>
<path d="M97.48,-87.99C98.14,-80.23 98.94,-70.91 99.68,-62.26"/>
<polygon points="103.17,-62.46 100.53,-52.2 96.2,-61.86 103.17,-62.46"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe3900017e0</title>
<ellipse cx="353" cy="-106.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="353" y="-102.58">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x7fe3900017e0&#45;&gt;p0x21989f0</title>
<path d="M351.52,-87.99C350.86,-80.23 350.06,-70.91 349.32,-62.26"/>
<polygon points="352.8,-61.86 348.47,-52.2 345.83,-62.46 352.8,-61.86"/>
</g>
<g class="m-node">
<title>p0x7fe390002000</title>
<polygon points="284.5,-210.77 229.5,-210.77 225.5,-206.77 225.5,-174.77 280.5,-174.77 284.5,-178.77 284.5,-210.77"/>
<polyline points="280.5,-206.77 225.5,-206.77 "/>
<polyline points="280.5,-206.77 280.5,-174.77 "/>
<polyline points="280.5,-206.77 284.5,-210.77 "/>
<text text-anchor="middle" x="255" y="-188.97">saxpy</text>
</g>
<g class="m-edge">
<title>p0x7fe390002000&#45;&gt;p0x2198ab0</title>
<path d="M255,-174.69C255,-163.26 255,-147.95 255,-134.82"/>
<polygon points="258.5,-134.44 255,-124.44 251.5,-134.44 258.5,-134.44"/>
</g>
</g>
</svg>
</div><p>The following code aggregates the five GPU operations using one cudaFlow and achieves better performance.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaflow</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">saxpy</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
                         <span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
  <span class="n">saxpy</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>
       <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>  <span class="c1">// creates one cudaFlow</span></pre><div class="m-graph"><svg style="width: 19.500rem; height: 6.250rem;" viewBox="0.00 0.00 311.53 99.77">
<g transform="scale(1 1) rotate(0) translate(4 95.77)">
<title>Taskflow</title>
<g class="m-node m-flat">
<title>p0x7f2870401a50</title>
<ellipse cx="43.13" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="43.13" y="-69.58">h2d_x</text>
</g>
<g class="m-node">
<title>p0x7f2870402bc0</title>
<polygon points="181.27,-63.38 126.27,-63.38 122.27,-59.38 122.27,-27.38 177.27,-27.38 181.27,-31.38 181.27,-63.38"/>
<polyline points="177.27,-59.38 122.27,-59.38 "/>
<polyline points="177.27,-59.38 177.27,-27.38 "/>
<polyline points="177.27,-59.38 181.27,-63.38 "/>
<text text-anchor="middle" x="151.77" y="-41.58">saxpy</text>
</g>
<g class="m-edge">
<title>p0x7f2870401a50&#45;&gt;p0x7f2870402bc0</title>
<path d="M80.23,-63.91C90.58,-61.19 101.9,-58.22 112.38,-55.46"/>
<polygon points="113.33,-58.83 122.11,-52.91 111.55,-52.06 113.33,-58.83"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402310</title>
<ellipse cx="260.4" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="260.4" y="-69.58">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402310</title>
<path d="M181.34,-52.89C191.17,-55.47 202.43,-58.43 213.29,-61.28"/>
<polygon points="212.53,-64.7 223.09,-63.85 214.31,-57.93 212.53,-64.7"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402780</title>
<ellipse cx="260.4" cy="-18.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="260.4" y="-14.58">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402780</title>
<path d="M181.34,-38.15C191.07,-35.69 202.22,-32.86 212.98,-30.14"/>
<polygon points="213.87,-33.52 222.7,-27.68 212.15,-26.74 213.87,-33.52"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870401eb0</title>
<ellipse cx="43.13" cy="-18.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="43.13" y="-14.58">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870401eb0&#45;&gt;p0x7f2870402bc0</title>
<path d="M80.53,-27.6C90.67,-30.17 101.71,-32.96 111.98,-35.56"/>
<polygon points="111.37,-39.02 121.93,-38.08 113.09,-32.24 111.37,-39.02"/>
</g>
</g>
</svg>
</div><aside class="m-note m-info"><h4>Note</h4><p>We encourage users to study and understand the parallel structure of their applications, in order to come up with the best granularity of task decomposition. A refined task graph can have significant performance difference from the raw counterpart.</p></aside></section><section id="OffloadAndUpdateAcudaFlow"><h2><a href="#OffloadAndUpdateAcudaFlow">Offload and Update a cudaFlow</a></h2><p>Many GPU applications require you to launch a cudaFlow multiple times and update node parameters (e.g., kernel parameters and memory addresses) between iterations. <a href="classtf_1_1cudaFlow.html#a85789ed8a1f47704cf1f1a2b98969444" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload</a> allows you to execute the graph immediately and then update the parameters for the next execution. When you offload a cudaFlow, an executable graph will be created, and you must NOT change the topology but the node parameters between successive executions.</p><pre class="m-code"><span class="mi">1</span><span class="o">:</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span> <span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">2</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">task</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">grid1</span><span class="p">,</span> <span class="n">block1</span><span class="p">,</span> <span class="n">shm1</span><span class="p">,</span> <span class="n">my_kernel</span><span class="p">,</span> <span class="n">args1</span><span class="p">...);</span>
<span class="mi">3</span><span class="o">:</span>   <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>  <span class="c1">// immediately run the cudaFlow once</span>
<span class="mi">4</span><span class="o">:</span>
<span class="mi">5</span><span class="o">:</span>   <span class="n">cf</span><span class="p">.</span><span class="n">update_kernel</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">grid2</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">shm2</span><span class="p">,</span> <span class="n">args2</span><span class="p">...);</span>
<span class="mi">6</span><span class="o">:</span>   <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>  <span class="c1">// run the cudaFlow again with the same graph topology</span>
<span class="mi">7</span><span class="o">:</span>                  <span class="c1">// but with different kernel parameters</span>
<span class="mi">8</span><span class="o">:</span> <span class="p">});</span></pre><p>Line 2 creates a kernel task to run <code>my_kernel</code> with the given parameters. Line 3 offloads the cudaFlow and performs an immediate execution; afterwards, we must not modify the graph topology. Line 5 updates the parameters of <code>my_kernel</code> associated with <code>task</code>. Line 6 executes the cudaFlow again with updated kernel parameters. We currently supports the following offload methods:</p><ul><li><a href="classtf_1_1cudaFlow.html#a85789ed8a1f47704cf1f1a2b98969444" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload</a> offloads and runs the cudaFlow once</li><li><a href="classtf_1_1cudaFlow.html#ac2269fd7dc8ca04a294a718204703dad" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload_n</a> offloads and runs the cudaFlow <code>n</code> times</li><li><a href="classtf_1_1cudaFlow.html#a99358da15e3bdfa1faabb3e326130e1f" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload_until</a> offloads and repeatedly runs the cudaFlow until the given predicate returns <code>true</code></li></ul><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// ... create GPU tasks</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>      <span class="c1">// offload the cudaFlow and run it once</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload_n</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span>  <span class="c1">// offload the cudaFlow and run it 10 times</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload_until</span><span class="p">([</span><span class="n">repeat</span><span class="o">=</span><span class="mi">5</span><span class="p">]</span> <span class="p">()</span> <span class="k">mutable</span> <span class="p">{</span> <span class="k">return</span> <span class="n">repeat</span><span class="o">--</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span> <span class="p">})</span>  <span class="c1">// 5 times</span>
<span class="p">};</span></pre><p>After you offload a cudaFlow (possibly multiple times), it is considered executed, and the executor will <em>not</em> run an offloaded cudaFlow after leaving the cudaFlow task callable. On the other hand, if a cudaFlow is not offloaded, the executor runs it once. For example, the following two versions represent the same execution logic.</p><pre class="m-code"><span class="c1">// version 1: explicitly offload a cudaFlow once</span>
<span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">shm</span><span class="p">,</span> <span class="n">my_kernel</span><span class="p">,</span> <span class="n">my_kernel_args</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;my_kernel&quot;</span><span class="p">);</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>
<span class="p">};</span>

<span class="c1">// version 2 (same as version 1): executor offloads the cudaFlow once</span>
<span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">shm</span><span class="p">,</span> <span class="n">my_kernel</span><span class="p">,</span> <span class="n">my_kernel_args</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;my_kernel&quot;</span><span class="p">);</span>
<span class="p">};</span></pre><p>Between successive offloads (i.e., executions of a cudaFlow), you can update the task parameters, such as changing the kernel execution parameters and memory operation parameters. We currently support the following methods to update task parameters from an offloaded cudaFlow:</p><ul><li><a href="classtf_1_1cudaFlow.html#abab3a11129e6286c1de3deecedae8090" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_kernel</a> updates the parameters of a kernel task</li><li><a href="classtf_1_1cudaFlow.html#a7972c77ba5f533b69e4b1dc55e87374d" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_copy</a> updates the parameters of a memcpy task to form a copy task</li><li><a href="classtf_1_1cudaFlow.html#af5f4cd1fc858a7725bbf57db629bdc34" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_memcpy</a> updates the parameters of a memcpy task</li><li><a href="classtf_1_1cudaFlow.html#a603072d44265de60647a7bcc5aaebace" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_memset</a> updates the parameters of a memset task</li><li><a href="classtf_1_1cudaFlow.html#a4a319c3e47fc538f6c31a7317c6a17e0" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_fill</a> updates the parameters of a memset task to form a fill task</li><li><a href="classtf_1_1cudaFlow.html#a62a042795e4a089ab633d809af6108a6" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_zero</a> updates the parameters of a memset task to form a zero task</li></ul><p>Please visit the reference page of <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> for more details.</p><aside class="m-note m-warning"><h4>Attention</h4><p>You must not change the topology of an offloaded graph. In addition, update methods have the following limitations:</p><ul><li>kernel task<ul><li>The kernel function is not allowed to change</li></ul></li><li>memset and memcpy tasks:<ul><li>The CUDA device(s) to which the operand(s) was allocated/mapped cannot change</li><li>The source/destination memory must be allocated from the same contexts as the original source/destination memory.</li></ul></li></ul></aside></section><section id="UsecudaFlowInAStandaloneEnvironment"><h2><a href="#UsecudaFlowInAStandaloneEnvironment">Use cudaFlow in a Standalone Environment</a></h2><p>You can use <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> in a standalone environment without going through <a href="classtf_1_1Taskflow.html" class="m-doc">tf::<wbr />Taskflow</a> and offloads it to GPU from the caller thread. All the features we have discussed so far are applicable for the standalone use.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span> <span class="n">cf</span><span class="p">;</span>  <span class="c1">// create a standalone cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">saxpy</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
                       <span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>

<span class="n">saxpy</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>   <span class="c1">// kernel runs after  host-to-device copy</span>
     <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span>  <span class="c1">// kernel runs before device-to-host copy</span>

<span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>  <span class="c1">// offload and run the standalone cudaFlow once</span></pre><aside class="m-note m-warning"><h4>Attention</h4><p>When using cudaFlow in a standalone environment, it is your choice to decide its GPU context.</p></aside><p>The following example creates a cudaFlow and executes it on GPU 0.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">cudaScopedDevice</span> <span class="n">gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span> <span class="n">cf</span><span class="p">;</span>  <span class="c1">// create a standalone cudaFlow on GPU 0</span>
<span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>     <span class="c1">// run the capturer once on GPU 0</span></pre></section>
      </div>
    </div>
  </div>
</article></main>
<div class="m-doc-search" id="search">
  <a href="#!" onclick="return hideSearch()"></a>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-m-8 m-push-m-2">
        <div class="m-doc-search-header m-text m-small">
          <div><span class="m-label m-default">Tab</span> / <span class="m-label m-default">T</span> to search, <span class="m-label m-default">Esc</span> to close</div>
          <div id="search-symbolcount">&hellip;</div>
        </div>
        <div class="m-doc-search-content">
          <form>
            <input type="search" name="q" id="search-input" placeholder="Loading &hellip;" disabled="disabled" autofocus="autofocus" autocomplete="off" spellcheck="false" />
          </form>
          <noscript class="m-text m-danger m-text-center">Unlike everything else in the docs, the search functionality <em>requires</em> JavaScript.</noscript>
          <div id="search-help" class="m-text m-dim m-text-center">
            <p class="m-noindent">Search for symbols, directories, files, pages or
            modules. You can omit any prefix from the symbol or file path; adding a
            <code>:</code> or <code>/</code> suffix lists all members of given symbol or
            directory.</p>
            <p class="m-noindent">Use <span class="m-label m-dim">&darr;</span>
            / <span class="m-label m-dim">&uarr;</span> to navigate through the list,
            <span class="m-label m-dim">Enter</span> to go.
            <span class="m-label m-dim">Tab</span> autocompletes common prefix, you can
            copy a link to the result using <span class="m-label m-dim">⌘</span>
            <span class="m-label m-dim">L</span> while <span class="m-label m-dim">⌘</span>
            <span class="m-label m-dim">M</span> produces a Markdown link.</p>
          </div>
          <div id="search-notfound" class="m-text m-warning m-text-center">Sorry, nothing was found.</div>
          <ul id="search-results"></ul>
        </div>
      </div>
    </div>
  </div>
</div>
<script src="search-v1.js"></script>
<script src="searchdata-v1.js" async="async"></script>
<footer><nav>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <p>Taskflow handbook is part of the <a href="https://taskflow.github.io">Taskflow project</a>, copyright © <a href="https://tsung-wei-huang.github.io/">Dr. Tsung-Wei Huang</a>, 2018&ndash;2021.<br />Generated by <a href="https://doxygen.org/">Doxygen</a> 1.8.20 and <a href="https://mcss.mosra.cz/">m.css</a>.</p>
      </div>
    </div>
  </div>
</nav></footer>
</body>
</html>
